分词器算法梳理：
1. 读入语料
      - 打开给定的训练文本（UTF‑8 编码），一次读入
  内存。
  2. 预分词（pretokenization）
      - 用 GPT‑2 那套正则把文本先切成较大的单元，比如
  词、数字、标点、空白等。
      - 如果有特殊 token（如 <|endoftext|>），要在正
  则里提前匹配，让它保持完整，不被拆开。
      - 统计每个预分词出现次数（比如用 Counter），但
  特殊 token 自身不参与统计。
  3. 初始化词表
      - 起始词表包含 256 个字节 token（0x00 到
  0xFF）。
      - 按参数把特殊 token 顺序追加到词表；这一步要检
  查总大小别超过 vocab_size。
  4. 把预分词转换为“字节序列+频次”
      - 每个预分词转成对应的字节序列（tuple），记录在
  字典里，值为频次。之后合并操作都基于这个结构。
  5. 迭代训练
      - 当词表还没满且有可合并的 pair：
          1. 遍历所有 token 的相邻字节对（pair），根
  据频次加权统计出现次数。
          2. 找到出现频率最高的 pair，就是下一个合并
  目标。
          3. 新 token 的字节串 = pair[0] 对应的字节拼
  接上 pair[1] 对应的字节。
          4. 把这个 pair 追加到 merges 列表，并给新
  token 分配词表 ID。
          5. 在所有 token 序列中，把 pair 替换成新
  ID（一次合并整对，只合最左起的匹配）。
          6. 更新“token 序列→频次”的字典，进入下
  一轮。
      - 如果没有任何 pair 可以合并（例如所有 token 长
  度都是 1），就提前结束。
  6. 返回结果
      - 最终返回 vocab（ID 到字节串的映射）和
  merges（按顺序记录的合并对列表）。

编码形式，
一段文本的编码，使用utf-8
关于utf-8编码，一种字节级映射，使得任何一串字符都被映射到某一串字节之中。
不过这里我想继续搞清楚关于unicode-utf-8-字符之间的映射关系：
任意一个字符对应一个码点（U+0041）
