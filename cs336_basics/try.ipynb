{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "167359e3",
   "metadata": {},
   "source": [
    "Jupyter Notebook在Mac上的常用快捷键：\n",
    "\n",
    "  命令模式快捷键（按Esc进入）：\n",
    "  - A - 在当前单元上方插入新单元\n",
    "  - B - 在当前单元下方插入新单元\n",
    "  - DD - 删除当前单元\n",
    "  - C - 复制单元\n",
    "  - V - 粘贴单元\n",
    "  - X - 剪切单元\n",
    "  - Z - 撤销删除单元\n",
    "  - Y - 将单元改为代码单元\n",
    "  - M - 将单元改为Markdown单元\n",
    "  - ⇧ + ↑/↓ - 选择多个单元\n",
    "  - ⇧ + M - 合并选中的单元\n",
    "\n",
    "  编辑模式快捷键（按Enter进入）：\n",
    "  - ⌘ + Enter - 运行当前单元\n",
    "  - ⇧ + Enter - 运行当前单元并选择下一个\n",
    "  - ⌥ + Enter - 运行当前单元并在下方插入新单元\n",
    "  - ⌘ + / - 注释/取消注释\n",
    "  - Tab - 代码补全\n",
    "  - ⇧ + Tab - 显示文档\n",
    "\n",
    "  通用快捷键：\n",
    "  - ⌘ + S - 保存笔记本\n",
    "  - ⌘ + Z - 撤销\n",
    "  - H - 显示快捷键帮助"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80c1d435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29275\n",
      "牛\n",
      "\u0000\n"
     ]
    }
   ],
   "source": [
    "print(ord('牛'))\n",
    "print(chr(29275))\n",
    "\n",
    "print(chr(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4njcpwbyd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question (a): What Unicode character does chr(0) return?\n",
    "print(\"chr(0):\", repr(chr(0)))\n",
    "print(\"Unicode name:\", ord(chr(0)))\n",
    "\n",
    "# Question (b): How does string representation differ from printed representation?\n",
    "print(\"\\nString representation (__repr__):\")\n",
    "print(repr(chr(0)))\n",
    "print(\"Printed representation:\")\n",
    "print(chr(0))\n",
    "\n",
    "# Question (c): What happens when this character occurs in text?\n",
    "print(\"\\nTesting chr(0) in text:\")\n",
    "test_string = \"this is a test\" + chr(0) + \"string\"\n",
    "print(\"repr of test string:\", repr(test_string))\n",
    "print(\"printed test string:\")\n",
    "print(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ajik1cdd8tu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unicode编码示例\n",
    "text = \"你好World\"\n",
    "print(f\"原始字符串: {text}\")\n",
    "\n",
    "# 编码为UTF-8字节\n",
    "utf8_bytes = text.encode('utf-8')\n",
    "print(f\"UTF-8字节: {utf8_bytes}\")\n",
    "\n",
    "# 获取字节值列表\n",
    "byte_values = list(utf8_bytes)\n",
    "print(f\"字节值: {byte_values}\")\n",
    "\n",
    "# 解码回字符串\n",
    "decoded = utf8_bytes.decode('utf-8')\n",
    "print(f\"解码后: {decoded}\")\n",
    "\n",
    "# 显示每个字符的码点\n",
    "for char in text:\n",
    "    print(f\"'{char}' -> Unicode码点: {ord(char)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "q1s85vlxwp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr(0): '\\x00'\n",
      "Unicode name: 0\n",
      "\n",
      "String representation (__repr__):\n",
      "'\\x00'\n",
      "Printed representation:\n",
      "\u0000\n",
      "\n",
      "Testing chr(0) in text:\n",
      "repr of test string: 'this is a test\\x00string'\n",
      "printed test string:\n",
      "this is a test\u0000string\n"
     ]
    }
   ],
   "source": [
    "# Question (a): What Unicode character does chr(0) return?\n",
    "print(\"chr(0):\", repr(chr(0)))\n",
    "print(\"Unicode name:\", ord(chr(0)))\n",
    "\n",
    "# Question (b): How does string representation differ from printed representation?\n",
    "print(\"\\nString representation (__repr__):\")\n",
    "print(repr(chr(0)))\n",
    "print(\"Printed representation:\")\n",
    "print(chr(0))\n",
    "\n",
    "# Question (c): What happens when this character occurs in text?\n",
    "print(\"\\nTesting chr(0) in text:\")\n",
    "test_string = \"this is a test\" + chr(0) + \"string\"\n",
    "print(\"repr of test string:\", repr(test_string))\n",
    "print(\"printed test string:\")\n",
    "print(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "oslf8v08n5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始字符串: 你好World\n",
      "UTF-8字节: b'\\xe4\\xbd\\xa0\\xe5\\xa5\\xbdWorld'\n",
      "字节值: [228, 189, 160, 229, 165, 189, 87, 111, 114, 108, 100]\n",
      "解码后: 你好World\n",
      "'你' -> Unicode码点: 20320\n",
      "'好' -> Unicode码点: 22909\n",
      "'W' -> Unicode码点: 87\n",
      "'o' -> Unicode码点: 111\n",
      "'r' -> Unicode码点: 114\n",
      "'l' -> Unicode码点: 108\n",
      "'d' -> Unicode码点: 100\n"
     ]
    }
   ],
   "source": [
    "# Unicode编码示例\n",
    "text = \"你好World\"\n",
    "print(f\"原始字符串: {text}\")\n",
    "\n",
    "# 编码为UTF-8字节\n",
    "utf8_bytes = text.encode('utf-8')\n",
    "print(f\"UTF-8字节: {utf8_bytes}\")\n",
    "\n",
    "# 获取字节值列表\n",
    "byte_values = list(utf8_bytes)\n",
    "print(f\"字节值: {byte_values}\")\n",
    "\n",
    "# 解码回字符串\n",
    "decoded = utf8_bytes.decode('utf-8')\n",
    "print(f\"解码后: {decoded}\")\n",
    "\n",
    "# 显示每个字符的码点\n",
    "for char in text:\n",
    "    print(f\"'{char}' -> Unicode码点: {ord(char)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a6565b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 b'\\x00'\n",
      "1 b'\\x01'\n",
      "2 b'\\x02'\n",
      "3 b'\\x03'\n",
      "4 b'\\x04'\n",
      "5 b'\\x05'\n",
      "6 b'\\x06'\n",
      "7 b'\\x07'\n",
      "8 b'\\x08'\n",
      "9 b'\\t'\n",
      "10 b'\\n'\n",
      "11 b'\\x0b'\n",
      "12 b'\\x0c'\n",
      "13 b'\\r'\n",
      "14 b'\\x0e'\n",
      "15 b'\\x0f'\n",
      "16 b'\\x10'\n",
      "17 b'\\x11'\n",
      "18 b'\\x12'\n",
      "19 b'\\x13'\n",
      "20 b'\\x14'\n",
      "21 b'\\x15'\n",
      "22 b'\\x16'\n",
      "23 b'\\x17'\n",
      "24 b'\\x18'\n",
      "25 b'\\x19'\n",
      "26 b'\\x1a'\n",
      "27 b'\\x1b'\n",
      "28 b'\\x1c'\n",
      "29 b'\\x1d'\n",
      "30 b'\\x1e'\n",
      "31 b'\\x1f'\n",
      "32 b' '\n",
      "33 b'!'\n",
      "34 b'\"'\n",
      "35 b'#'\n",
      "36 b'$'\n",
      "37 b'%'\n",
      "38 b'&'\n",
      "39 b\"'\"\n",
      "40 b'('\n",
      "41 b')'\n",
      "42 b'*'\n",
      "43 b'+'\n",
      "44 b','\n",
      "45 b'-'\n",
      "46 b'.'\n",
      "47 b'/'\n",
      "48 b'0'\n",
      "49 b'1'\n",
      "50 b'2'\n",
      "51 b'3'\n",
      "52 b'4'\n",
      "53 b'5'\n",
      "54 b'6'\n",
      "55 b'7'\n",
      "56 b'8'\n",
      "57 b'9'\n",
      "58 b':'\n",
      "59 b';'\n",
      "60 b'<'\n",
      "61 b'='\n",
      "62 b'>'\n",
      "63 b'?'\n",
      "64 b'@'\n",
      "65 b'A'\n",
      "66 b'B'\n",
      "67 b'C'\n",
      "68 b'D'\n",
      "69 b'E'\n",
      "70 b'F'\n",
      "71 b'G'\n",
      "72 b'H'\n",
      "73 b'I'\n",
      "74 b'J'\n",
      "75 b'K'\n",
      "76 b'L'\n",
      "77 b'M'\n",
      "78 b'N'\n",
      "79 b'O'\n",
      "80 b'P'\n",
      "81 b'Q'\n",
      "82 b'R'\n",
      "83 b'S'\n",
      "84 b'T'\n",
      "85 b'U'\n",
      "86 b'V'\n",
      "87 b'W'\n",
      "88 b'X'\n",
      "89 b'Y'\n",
      "90 b'Z'\n",
      "91 b'['\n",
      "92 b'\\\\'\n",
      "93 b']'\n",
      "94 b'^'\n",
      "95 b'_'\n",
      "96 b'`'\n",
      "97 b'a'\n",
      "98 b'b'\n",
      "99 b'c'\n",
      "100 b'd'\n",
      "101 b'e'\n",
      "102 b'f'\n",
      "103 b'g'\n",
      "104 b'h'\n",
      "105 b'i'\n",
      "106 b'j'\n",
      "107 b'k'\n",
      "108 b'l'\n",
      "109 b'm'\n",
      "110 b'n'\n",
      "111 b'o'\n",
      "112 b'p'\n",
      "113 b'q'\n",
      "114 b'r'\n",
      "115 b's'\n",
      "116 b't'\n",
      "117 b'u'\n",
      "118 b'v'\n",
      "119 b'w'\n",
      "120 b'x'\n",
      "121 b'y'\n",
      "122 b'z'\n",
      "123 b'{'\n",
      "124 b'|'\n",
      "125 b'}'\n",
      "126 b'~'\n",
      "127 b'\\x7f'\n",
      "128 b'\\x80'\n",
      "129 b'\\x81'\n",
      "130 b'\\x82'\n",
      "131 b'\\x83'\n",
      "132 b'\\x84'\n",
      "133 b'\\x85'\n",
      "134 b'\\x86'\n",
      "135 b'\\x87'\n",
      "136 b'\\x88'\n",
      "137 b'\\x89'\n",
      "138 b'\\x8a'\n",
      "139 b'\\x8b'\n",
      "140 b'\\x8c'\n",
      "141 b'\\x8d'\n",
      "142 b'\\x8e'\n",
      "143 b'\\x8f'\n",
      "144 b'\\x90'\n",
      "145 b'\\x91'\n",
      "146 b'\\x92'\n",
      "147 b'\\x93'\n",
      "148 b'\\x94'\n",
      "149 b'\\x95'\n",
      "150 b'\\x96'\n",
      "151 b'\\x97'\n",
      "152 b'\\x98'\n",
      "153 b'\\x99'\n",
      "154 b'\\x9a'\n",
      "155 b'\\x9b'\n",
      "156 b'\\x9c'\n",
      "157 b'\\x9d'\n",
      "158 b'\\x9e'\n",
      "159 b'\\x9f'\n",
      "160 b'\\xa0'\n",
      "161 b'\\xa1'\n",
      "162 b'\\xa2'\n",
      "163 b'\\xa3'\n",
      "164 b'\\xa4'\n",
      "165 b'\\xa5'\n",
      "166 b'\\xa6'\n",
      "167 b'\\xa7'\n",
      "168 b'\\xa8'\n",
      "169 b'\\xa9'\n",
      "170 b'\\xaa'\n",
      "171 b'\\xab'\n",
      "172 b'\\xac'\n",
      "173 b'\\xad'\n",
      "174 b'\\xae'\n",
      "175 b'\\xaf'\n",
      "176 b'\\xb0'\n",
      "177 b'\\xb1'\n",
      "178 b'\\xb2'\n",
      "179 b'\\xb3'\n",
      "180 b'\\xb4'\n",
      "181 b'\\xb5'\n",
      "182 b'\\xb6'\n",
      "183 b'\\xb7'\n",
      "184 b'\\xb8'\n",
      "185 b'\\xb9'\n",
      "186 b'\\xba'\n",
      "187 b'\\xbb'\n",
      "188 b'\\xbc'\n",
      "189 b'\\xbd'\n",
      "190 b'\\xbe'\n",
      "191 b'\\xbf'\n",
      "192 b'\\xc0'\n",
      "193 b'\\xc1'\n",
      "194 b'\\xc2'\n",
      "195 b'\\xc3'\n",
      "196 b'\\xc4'\n",
      "197 b'\\xc5'\n",
      "198 b'\\xc6'\n",
      "199 b'\\xc7'\n",
      "200 b'\\xc8'\n",
      "201 b'\\xc9'\n",
      "202 b'\\xca'\n",
      "203 b'\\xcb'\n",
      "204 b'\\xcc'\n",
      "205 b'\\xcd'\n",
      "206 b'\\xce'\n",
      "207 b'\\xcf'\n",
      "208 b'\\xd0'\n",
      "209 b'\\xd1'\n",
      "210 b'\\xd2'\n",
      "211 b'\\xd3'\n",
      "212 b'\\xd4'\n",
      "213 b'\\xd5'\n",
      "214 b'\\xd6'\n",
      "215 b'\\xd7'\n",
      "216 b'\\xd8'\n",
      "217 b'\\xd9'\n",
      "218 b'\\xda'\n",
      "219 b'\\xdb'\n",
      "220 b'\\xdc'\n",
      "221 b'\\xdd'\n",
      "222 b'\\xde'\n",
      "223 b'\\xdf'\n",
      "224 b'\\xe0'\n",
      "225 b'\\xe1'\n",
      "226 b'\\xe2'\n",
      "227 b'\\xe3'\n",
      "228 b'\\xe4'\n",
      "229 b'\\xe5'\n",
      "230 b'\\xe6'\n",
      "231 b'\\xe7'\n",
      "232 b'\\xe8'\n",
      "233 b'\\xe9'\n",
      "234 b'\\xea'\n",
      "235 b'\\xeb'\n",
      "236 b'\\xec'\n",
      "237 b'\\xed'\n",
      "238 b'\\xee'\n",
      "239 b'\\xef'\n",
      "240 b'\\xf0'\n",
      "241 b'\\xf1'\n",
      "242 b'\\xf2'\n",
      "243 b'\\xf3'\n",
      "244 b'\\xf4'\n",
      "245 b'\\xf5'\n",
      "246 b'\\xf6'\n",
      "247 b'\\xf7'\n",
      "248 b'\\xf8'\n",
      "249 b'\\xf9'\n",
      "250 b'\\xfa'\n",
      "251 b'\\xfb'\n",
      "252 b'\\xfc'\n",
      "253 b'\\xfd'\n",
      "254 b'\\xfe'\n",
      "255 b'\\xff'\n"
     ]
    }
   ],
   "source": [
    "for i in range(256):\n",
    "    print(i,bytes([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbc1de0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello! \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf!'\n"
     ]
    }
   ],
   "source": [
    "test_string = \"hello! こんにちは!\"\n",
    "utf8_encoded = test_string.encode(\"utf-8\")\n",
    "print(utf8_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23dc8800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81d5406e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['some', ' text', ' that', ' i', \"'ll\", ' pre', '-', 'tokenize']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "re.findall(PAT, \"some text that i'll pre-tokenize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "gbdcmn5c5un",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern breakdown:\n",
      "1. '(?:[sdmt]|ll|ve|re)  - Contractions like 'll, 've, 're, 's, 'd, 'm, 't\n",
      "2. ?\\p{L}+               - Optional space + Unicode letters\n",
      "3. ?\\p{N}+               - Optional space + Unicode numbers\n",
      "4. ?[^\\s\\p{L}\\p{N}]+     - Optional space + non-letter/number/space chars\n",
      "5. \\s+(?!\\S)             - Trailing spaces\n",
      "6. \\s+                   - Any whitespace\n",
      "\n",
      "Tokenization result:\n",
      "['some', ' text', ' that', ' i', \"'ll\", ' pre', '-', 'tokenize']\n",
      "\n",
      "Tokens with quotes for clarity:\n",
      "[\"'some'\", \"' text'\", \"' that'\", \"' i'\", \"''ll'\", \"' pre'\", \"'-'\", \"'tokenize'\"]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "# Python's re module doesn't support \\p{L} and \\p{N} Unicode categories\n",
    "# Let's use the regex module instead, or demonstrate with a simpler example\n",
    "test_text = \"some text that i'll pre-tokenize\"\n",
    "\n",
    "# For demonstration, let's break down what this pattern is trying to match\n",
    "print(\"Pattern breakdown:\")\n",
    "print(\"1. '(?:[sdmt]|ll|ve|re)  - Contractions like 'll, 've, 're, 's, 'd, 'm, 't\")\n",
    "print(\"2. ?\\p{L}+               - Optional space + Unicode letters\")\n",
    "print(\"3. ?\\p{N}+               - Optional space + Unicode numbers\")\n",
    "print(\"4. ?[^\\s\\p{L}\\p{N}]+     - Optional space + non-letter/number/space chars\")\n",
    "print(\"5. \\s+(?!\\S)             - Trailing spaces\")\n",
    "print(\"6. \\s+                   - Any whitespace\")\n",
    "\n",
    "# Since standard re doesn't support \\p{}, let's use a simplified version\n",
    "simplified_pat = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?[a-zA-Z]+| ?[0-9]+| ?[^\\s\\w]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "result = re.findall(simplified_pat, test_text)\n",
    "print(\"\\nTokenization result:\")\n",
    "print(result)\n",
    "print(\"\\nTokens with quotes for clarity:\")\n",
    "print([f\"'{token}'\" for token in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ufhav4lny8g",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入文本: \"some text that i'll pre-tokenize\"\n",
      "\n",
      "分词结果:\n",
      "  Token 0: 'some'\n",
      "  Token 1: ' text'\n",
      "  Token 2: ' that'\n",
      "  Token 3: ' i'\n",
      "  Token 4: ''ll'\n",
      "  Token 5: ' pre'\n",
      "  Token 6: '-'\n",
      "  Token 7: 'tokenize'\n",
      "\n",
      "拼接验证: True\n"
     ]
    }
   ],
   "source": [
    "# 使用简化版模式演示（因为标准re不支持\\p{}）\n",
    "import re\n",
    "\n",
    "# 模拟GPT风格的预分词模式\n",
    "simplified_pat = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?[a-zA-Z]+| ?[0-9]+| ?[^\\s\\w]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "text = \"some text that i'll pre-tokenize\"\n",
    "tokens = re.findall(simplified_pat, text)\n",
    "\n",
    "print(\"输入文本:\", repr(text))\n",
    "print(\"\\n分词结果:\")\n",
    "for i, token in enumerate(tokens):\n",
    "    print(f\"  Token {i}: '{token}'\")\n",
    "\n",
    "print(\"\\n拼接验证:\", ''.join(tokens) == text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eg7s12yc21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找数字: ['123', '456']\n",
      "找字母: ['abc', 'def', 'ghi']\n",
      "分词结果: ['abc', '123', 'def', '456', 'ghi']\n",
      "\n",
      "关键区别:\n",
      "- findall 只是'查找所有匹配'\n",
      "- 能否分词取决于正则表达式是否覆盖全文\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# re.findall 的基本用法示例\n",
    "text = \"abc123def456ghi\"\n",
    "\n",
    "# 示例1：找所有数字\n",
    "numbers = re.findall(r'\\d+', text)\n",
    "print(\"找数字:\", numbers)  # ['123', '456']\n",
    "\n",
    "# 示例2：找所有字母序列\n",
    "letters = re.findall(r'[a-z]+', text)\n",
    "print(\"找字母:\", letters)  # ['abc', 'def', 'ghi']\n",
    "\n",
    "# 示例3：用于分词 - 需要设计模式覆盖所有字符\n",
    "tokenize_pattern = r'[a-z]+|\\d+'  # 字母或数字\n",
    "tokens = re.findall(tokenize_pattern, text)\n",
    "print(\"分词结果:\", tokens)  # ['abc', '123', 'def', '456', 'ghi']\n",
    "\n",
    "print(\"\\n关键区别:\")\n",
    "print(\"- findall 只是'查找所有匹配'\")\n",
    "print(\"- 能否分词取决于正则表达式是否覆盖全文\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2n0f8f4govc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 不完整的模式 ===\n",
      "模式: [a-z]+\n",
      "结果: ['hello', 'world']\n",
      "丢失了: '-'\n",
      "\n",
      "=== 完整覆盖的模式 ===\n",
      "模式: [a-z]+|[^a-z]+\n",
      "结果: ['hello', '-', 'world']\n",
      "完整覆盖: True\n",
      "\n",
      "=== GPT的PAT模式 ===\n",
      "设计原则:\n",
      "1. 用 | 列举所有可能的token类型\n",
      "2. 确保每个字符都能被某个子模式匹配\n",
      "3. 按优先级排序（缩写 > 单词 > 标点 > 空格）\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"hello-world\"\n",
    "\n",
    "# 对比：不完整的模式 vs 完整覆盖的模式\n",
    "print(\"=== 不完整的模式 ===\")\n",
    "partial_pattern = r'[a-z]+'  # 只匹配字母\n",
    "result1 = re.findall(partial_pattern, text)\n",
    "print(f\"模式: {partial_pattern}\")\n",
    "print(f\"结果: {result1}\")\n",
    "print(f\"丢失了: '-'\")\n",
    "\n",
    "print(\"\\n=== 完整覆盖的模式 ===\")\n",
    "full_pattern = r'[a-z]+|[^a-z]+'  # 字母 或 非字母\n",
    "result2 = re.findall(full_pattern, text)\n",
    "print(f\"模式: {full_pattern}\")\n",
    "print(f\"结果: {result2}\")\n",
    "print(f\"完整覆盖: {''.join(result2) == text}\")\n",
    "\n",
    "print(\"\\n=== GPT的PAT模式 ===\")\n",
    "print(\"设计原则:\")\n",
    "print(\"1. 用 | 列举所有可能的token类型\")\n",
    "print(\"2. 确保每个字符都能被某个子模式匹配\")\n",
    "print(\"3. 按优先级排序（缩写 > 单词 > 标点 > 空格）\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ijarno7pl6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 不同正则模式的效果 ===\n",
      "\n",
      "模式1 (只找字母): [A-Za-z]+\n",
      "结果: ['I', 'll', 'eat', 'apples']\n",
      "问题: 丢失了缩写、数字、标点\n",
      "\n",
      "模式2 (简单分词): \\w+|[^\\w\\s]\n",
      "结果: ['I', \"'\", 'll', 'eat', '3', 'apples', '!']\n",
      "问题: 缩写被错误处理\n",
      "\n",
      "模式3 (GPT风格): '(?:ll|s|t)|\\w+|[^\\w\\s]\n",
      "结果: ['I', \"'ll\", 'eat', '3', 'apples', '!']\n",
      "优点: 正确识别缩写 'll\n",
      "\n",
      "结论：\n",
      "• re.findall 只是工具\n",
      "• 分词效果完全取决于正则表达式的设计\n",
      "• GPT的PAT是一个精心设计的'完备'模式\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 演示：同样的 findall，不同的模式，不同的效果\n",
    "text = \"I'll eat 3 apples!\"\n",
    "\n",
    "print(\"=== 不同正则模式的效果 ===\\n\")\n",
    "\n",
    "# 1. 只找字母\n",
    "pattern1 = r'[A-Za-z]+'\n",
    "print(f\"模式1 (只找字母): {pattern1}\")\n",
    "print(f\"结果: {re.findall(pattern1, text)}\")\n",
    "print(f\"问题: 丢失了缩写、数字、标点\\n\")\n",
    "\n",
    "# 2. 简单分词\n",
    "pattern2 = r'\\w+|[^\\w\\s]'\n",
    "print(f\"模式2 (简单分词): {pattern2}\")\n",
    "print(f\"结果: {re.findall(pattern2, text)}\")\n",
    "print(f\"问题: 缩写被错误处理\\n\")\n",
    "\n",
    "# 3. GPT风格分词（简化版）\n",
    "pattern3 = r\"'(?:ll|s|t)|\\w+|[^\\w\\s]\"\n",
    "print(f\"模式3 (GPT风格): {pattern3}\")\n",
    "print(f\"结果: {re.findall(pattern3, text)}\")\n",
    "print(f\"优点: 正确识别缩写 'll\\n\")\n",
    "\n",
    "print(\"结论：\")\n",
    "print(\"• re.findall 只是工具\")\n",
    "print(\"• 分词效果完全取决于正则表达式的设计\")\n",
    "print(\"• GPT的PAT是一个精心设计的'完备'模式\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "yrkl077ems",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== re.findall() ===\n",
      "返回类型: <class 'list'>\n",
      "返回内容: 列表，包含 501 个字符串\n",
      "内存占用: 约 4216 字节\n",
      "前3个: ['hello', 'world', 'hello']\n",
      "\n",
      "=== re.finditer() ===\n",
      "返回类型: <class 'callable_iterator'>\n",
      "内存占用: 约 48 字节（只是迭代器）\n",
      "前3个: ['hello', 'world', 'hello']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "\n",
    "text = \"hello world, hello python, hello AI\" * 100  # 模拟大文本\n",
    "\n",
    "pattern = r'\\w+'\n",
    "\n",
    "print(\"=== re.findall() ===\")\n",
    "# findall 立即返回所有匹配的字符串列表\n",
    "all_matches = re.findall(pattern, text)\n",
    "print(f\"返回类型: {type(all_matches)}\")\n",
    "print(f\"返回内容: 列表，包含 {len(all_matches)} 个字符串\")\n",
    "print(f\"内存占用: 约 {sys.getsizeof(all_matches)} 字节\")\n",
    "print(f\"前3个: {all_matches[:3]}\")\n",
    "\n",
    "print(\"\\n=== re.finditer() ===\")\n",
    "# finditer 返回迭代器，按需生成匹配对象\n",
    "iter_matches = re.finditer(pattern, text)\n",
    "print(f\"返回类型: {type(iter_matches)}\")\n",
    "print(f\"内存占用: 约 {sys.getsizeof(iter_matches)} 字节（只是迭代器）\")\n",
    "\n",
    "# 使用迭代器时才真正处理\n",
    "first_three = []\n",
    "for i, match in enumerate(iter_matches):\n",
    "    if i < 3:\n",
    "        first_three.append(match.group())  # 获取匹配的字符串\n",
    "    if i >= 2:\n",
    "        break\n",
    "print(f\"前3个: {first_three}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "yg8ym3y8vir",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 方法1: findall() - 不推荐 ===\n",
      "步骤1: 创建了 9000 个字符串的列表\n",
      "步骤2: 统计得到 9 个唯一token\n",
      "\n",
      "=== 方法2: finditer() - 推荐 ===\n",
      "结果: 统计得到 9 个唯一token\n",
      "没有创建中间列表！\n",
      "\n",
      "=== 内存效率对比 ===\n",
      "findall: 需要存储 9000 个token副本\n",
      "finditer: 只需要存储 9 个唯一token的计数\n",
      "节省比例: 99.9%\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# 模拟一个大文本语料库\n",
    "corpus = \"The quick brown fox jumps over the lazy dog. \" * 1000\n",
    "pattern = r'\\w+'\n",
    "\n",
    "print(\"=== 方法1: findall() - 不推荐 ===\")\n",
    "# 先创建所有token的列表，再统计\n",
    "all_tokens = re.findall(pattern, corpus)  # ❌ 存储了所有token\n",
    "token_counts_1 = Counter(all_tokens)      # 然后才统计\n",
    "print(f\"步骤1: 创建了 {len(all_tokens)} 个字符串的列表\")\n",
    "print(f\"步骤2: 统计得到 {len(token_counts_1)} 个唯一token\")\n",
    "\n",
    "print(\"\\n=== 方法2: finditer() - 推荐 ===\")\n",
    "# 直接在迭代中统计，不存储中间列表\n",
    "token_counts_2 = Counter()\n",
    "for match in re.finditer(pattern, corpus):  # ✅ 流式处理\n",
    "    token = match.group()\n",
    "    token_counts_2[token] += 1              # 直接更新计数\n",
    "\n",
    "print(f\"结果: 统计得到 {len(token_counts_2)} 个唯一token\")\n",
    "print(f\"没有创建中间列表！\")\n",
    "\n",
    "print(\"\\n=== 内存效率对比 ===\")\n",
    "print(f\"findall: 需要存储 {len(all_tokens)} 个token副本\")\n",
    "print(f\"finditer: 只需要存储 {len(token_counts_2)} 个唯一token的计数\")\n",
    "print(f\"节省比例: {(1 - len(token_counts_2)/len(all_tokens))*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2addcc8",
   "metadata": {},
   "source": [
    "现在，我们已将输入文本转换为预分词，并用UTF-8字节序列表示每个预分词，接下来可以计算BPE合并操作（即训练BPE分词器）。从高层次来看，BPE算法会反复统计每一对字节的出现频率，并找出其中频率最高的那对（例如，“A”和“B”）。随后，所有出现频率最高的这对字节（“A”和“B”）都会被合并——即替换为一个新的标记“AB”。这一新合并的标记会被加入我们的词汇表中。因此，经过BPE训练后，最终的词汇表大小将是初始词汇表的大小（本例中为256），再加上训练过程中执行的BPE合并操作次数。为了提高BPE训练的效率，我们不会考虑那些跨越预分词边界的情况。在计算合并时，若遇到多个频率相同的字节对，我们将以字典序较大的那对为准，优先进行合并。例如，如果字节对（“A”、“B”）、（“A”、“C”）、（“B”、“ZZ”）以及（“BA”、“A”）的频率均最高，那么我们会选择合并“BA”和“A”："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71f88ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('BA', 'A')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([(\"A\", \"B\"), (\"A\", \"C\"), (\"B\", \"ZZ\"), (\"BA\", \"A\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56482dd8",
   "metadata": {},
   "source": [
    "特殊标记 通常，一些字符串（例如，）被用于编码元数据（例如，<|endoftext|>）。在对文本进行编码时，我们往往希望将某些字符串视为“特殊标记”，这些标记绝不会被拆分为多个子标记（即始终作为单个标记保留）。例如，序列结束字符串就应始终作为一个单独的标记（即单一的整数ID）保留，以便我们能够准确判断何时停止从语言模型中生成内容。因此，这些特殊标记必须被添加到词汇表中，从而为其分配一个固定的标记ID。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b354e22",
   "metadata": {},
   "source": [
    "### Example (bpe_example): BPE training example\n",
    "Here is a stylized example from Sennrich et al. [2016]. Consider a corpus consisting of the following text  low low low low low lower lower widest widest widest newest newest newest newest newest newest  and the vocabulary has a special token <|endoftext|>.  Vocabulary We initialize our vocabulary with our special token <|endoftext|> and the 256 byte values.  \n",
    "\n",
    "Pre-tokenization For simplicity and to focus on the merge procedure, we assume in this example that pretokenization simply splits on whitespace. When we pretokenize and count, we end up with the frequency table.\n",
    "\n",
    "{low: 5, lower: 2, widest: 3, newest: 6}\n",
    "\n",
    "It is convenient to represent this as a dict[tuple[bytes], int], e.g. {(l,o,w): 5 ...}. Note that even a single byte is a bytes object in Python. There is no byte type in Python to represent a single byte, just as there is no char type in Python to represent a single character.\n",
    "\n",
    "Merges We first look at every successive pair of bytes and sum the frequency of the words where they appear {lo: 7, ow: 7, we: 8, er: 2, wi: 3, id: 3, de: 3, es: 9, st: 9, ne: 6, ew: 6}. The pair ('es') and ('st') are tied, so we take the lexicographically greater pair, ('st'). We would then merge the pre-tokens so that we end up with {(l,o,w): 5, (l,o,w,e,r): 2, (w,i,d,e,st): 3, (n,e,w,e,st): 6}. \n",
    "In the second round, we see that (e, st) is the most common pair (with a count of 9) and we would merge into {(l,o,w): 5, (l,o,w,e,r): 2, (w,i,d,est): 3, (n,e,w,est): 6}. Continuing this, the sequence of merges we get in the end will be ['s t', 'e st', 'o w', 'l ow', 'w est', 'n e', 'ne west', 'w i', 'wi d', 'wid est', 'low e', 'lowe r']. \n",
    "If we take 6 merges, we have ['s t', 'e st', 'o w', 'l ow', 'w est', 'n e'] and our vocabulary elements would be [<|endoftext|>, [...256 BYTE CHARS], st, est, ow, low, west, ne]. \n",
    "With this vocabulary and set of merges, the word newest would tokenize as [ne, west]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cf5b768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(b'l', b'o'): 5, (b'o', b'w'): 5, (b'n', b'e'): 6, (b'e', b'w'): 6, (b'w', b'e'): 6, (b'e', b's'): 6, (b's', b't'): 6}\n"
     ]
    }
   ],
   "source": [
    "def get_pair_stats(splits):\n",
    "    \"\"\"\n",
    "    输入一个拆分好的单词频率字典，\n",
    "    返回一个相邻字节对的频率字典。\n",
    "    \"\"\"\n",
    "    counts = {} # 这就是我们的小本本\n",
    "    \n",
    "    # 遍历每一个单词（字节元组）和它的频率\n",
    "    for word_tuple, freq in splits.items():\n",
    "        # 遍历这个单词元组，找出所有相邻的对\n",
    "        # range(len(word_tuple) - 1) 能确保我们不会取到最后一个元素的后面\n",
    "        for i in range(len(word_tuple) - 1):\n",
    "            # 提取相邻的对，例如 ('l', 'o')\n",
    "            pair = (word_tuple[i], word_tuple[i+1])\n",
    "            \n",
    "            # 在我们的小本本上更新这个对的频率\n",
    "            # dict.get(pair, 0) 的意思是：如果counts里已经有pair这个键，\n",
    "            # 就返回它的值，否则返回0。这样写很安全。\n",
    "            counts[pair] = counts.get(pair, 0) + freq\n",
    "            \n",
    "    return counts\n",
    "\n",
    "# --- 测试一下 ---\n",
    "# 我们的输入数据 (注意，键已经是元组了)\n",
    "splits_data = {\n",
    "    (b'l', b'o', b'w'): 5,\n",
    "    (b'n', b'e', b'w', b'e', b's', b't'): 6\n",
    "}\n",
    "\n",
    "# 调用函数\n",
    "pair_frequencies = get_pair_stats(splits_data)\n",
    "\n",
    "# 打印结果看看\n",
    "print(pair_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1c39a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合并前的数据: {(b'l', b'o', b'w'): 5, (b'n', b'e', b'w', b'e', b's', b't'): 6}\n",
      "合并后的数据: {(b'l', b'o', b'w'): 5, (b'n', b'e', b'w', b'es', b't'): 6}\n"
     ]
    }
   ],
   "source": [
    "def merge_pair(splits, pair_to_merge):\n",
    "    \"\"\"\n",
    "    输入原始单词列表和要合并的对，返回一个新的合并后的单词列表。\n",
    "    \"\"\"\n",
    "    new_splits = {}\n",
    "    \n",
    "    # 遍历每一个单词（字节元组）和它的频率\n",
    "    for word_tuple, freq in splits.items():\n",
    "        \n",
    "        # --- 在这个单词里执行替换操作 ---\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word_tuple):\n",
    "            # 检查当前位置和下一个位置的token是否是我们要合并的对\n",
    "            # i < len(word_tuple) - 1 是为了防止索引越界\n",
    "            if i < len(word_tuple) - 1 and (word_tuple[i], word_tuple[i+1]) == pair_to_merge:\n",
    "                # 如果是，就添加合并后的新token\n",
    "                new_token = pair_to_merge[0] + pair_to_merge[1]\n",
    "                new_word.append(new_token)\n",
    "                i += 2 # 跳过两个已经处理过的旧token\n",
    "            else:\n",
    "                # 如果不是，就照原样添加当前的token\n",
    "                new_word.append(word_tuple[i])\n",
    "                i += 1 # 处理下一个token\n",
    "        \n",
    "        # 将新生成的单词（列表形式）转换回元组，并放入新的字典中\n",
    "        new_splits[tuple(new_word)] = freq\n",
    "        \n",
    "    return new_splits\n",
    "\n",
    "# --- 测试一下 ---\n",
    "splits_data = {\n",
    "    (b'l', b'o', b'w'): 5,\n",
    "    (b'n', b'e', b'w', b'e', b's', b't'): 6\n",
    "}\n",
    "best_pair = (b'e', b's') # 假设我们已经找到了这个赢家\n",
    "\n",
    "# 调用合并函数\n",
    "updated_splits = merge_pair(splits_data, best_pair)\n",
    "\n",
    "print(\"合并前的数据:\", splits_data)\n",
    "print(\"合并后的数据:\", updated_splits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
