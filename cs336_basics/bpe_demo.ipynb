{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67f95c60",
   "metadata": {},
   "source": [
    "  - Step 1 – Set Up Playground: 在 bpe_demo.ipynb 里\n",
    "  新增一个“Toy corpus”单元，放极小语料（如 [\"low\",\n",
    "  \"lower\", \"newest\", \"widest\"]），再建一个 Markdown\n",
    "  单元写清楚接下来每个函数的目标。\n",
    "  - Step 2 – tokenize_to_chars: 写第一个 code cell，\n",
    "  实现并调用 tokenize_to_chars(corpus)，打印/展示结\n",
    "  果，确保得到逐字符或逐字节的列表结构。\n",
    "  - Step 3 – count_adjacent_pairs: 新建 cell，实\n",
    "  现统计函数；对上一步的输出运行，打印 pair 频次\n",
    "  Counter，核对与手算一致。\n",
    "  - Step 4 – merge_pair: 单独写一个 cell 实现“把\n",
    "  best_pair 合并进所有序列”的逻辑；调用时用固定\n",
    "  best_pair，展示合并前后 tokens，确认不会产生重叠\n",
    "  问题。\n",
    "  - Step 5 – argmax & loop glue: 写一个 cell 实现\n",
    "  argmax（或直接用 Python 内置）并把前三个函数串成一\n",
    "  次循环，手动跑 1~2 次，观察 merges 记录。\n",
    "  - Step 6 – build_vocab: 写最终构造词表的函数，展示\n",
    "  vocab 内容（可打印前若干项）。\n",
    "  - Step 7 – Wrap train_bpe: 最后把上面的函数组合成完\n",
    "  整 train_bpe，对 toy corpus 和 max_merges 运行，打\n",
    "  印 merges 和 vocab；确认与预期一致后再考虑扩展到真\n",
    "  实语料。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flow-pseudocode-map",
   "metadata": {},
   "source": [
    "### Flow ↔ Pseudocode Mapping\n",
    "\n",
    "- `tokenize_to_chars` → “初始分词：字符 / 字节”\n",
    "- `count_adjacent_pairs` → “统计全部相邻 token pair 的频次”\n",
    "- `argmax(pair_counts)` → “挑选频次最高的 pair”\n",
    "- `merge_pair(tokens, best_pair)` → “合并 pair → 生成新 token”和“更新语料中的 token 序列”\n",
    "- `merges.append(best_pair)` → “记录合并步骤到 merges”\n",
    "- `if not pair_counts: break` 与 `for` 循环结束 → “达到限制?”\n",
    "- `build_vocab(tokens, merges)` → “输出 merges + vocab”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7190dfd6",
   "metadata": {},
   "source": [
    "### Step 1 · Toy Corpus Setup\n",
    "    We start with a tiny corpus（语料库） so every intermediate\n",
    "  result is easy to inspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83d782c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['low', 'lower', 'newest', 'widest']\n"
     ]
    }
   ],
   "source": [
    "toy_corpus = [\"low\", \"lower\", \"newest\", \"widest\"]\n",
    "\n",
    "print(toy_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572ca595",
   "metadata": {},
   "source": [
    "### Step 2 · tokenize_to_chars\n",
    "    Convert each text sample into a list of base\n",
    "  tokens (characters or bytes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "703865f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low -> ['l', 'o', 'w']\n",
      "lower -> ['l', 'o', 'w', 'e', 'r']\n",
      "newest -> ['n', 'e', 'w', 'e', 's', 't']\n",
      "widest -> ['w', 'i', 'd', 'e', 's', 't']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_to_chars(corpus: list[str]) -> list[list[str]]:\n",
    "    \"\"\" \"\n",
    "      Break each piece of text into a list of\n",
    "    single-character tokens.\n",
    "    \"\"\"\n",
    "    tokenized = []\n",
    "    for text in corpus:\n",
    "        tokenized.append(list(text))\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "toy_tokens = tokenize_to_chars(toy_corpus)\n",
    "for sample, tokens in zip(toy_corpus, toy_tokens):\n",
    "    print(sample, \"->\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9ee449",
   "metadata": {},
   "source": [
    "tokenized corpus now holds per-word cahracter lists;this will feed the pair counter next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc88ffd",
   "metadata": {},
   "source": [
    "### Step 3 · count_adjacent_pairs\n",
    "    Count how often each adjacent token pair appears across the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f9189d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('l', 'o') : 2\n",
      "('o', 'w') : 2\n",
      "('w', 'e') : 2\n",
      "('e', 's') : 2\n",
      "('s', 't') : 2\n",
      "('e', 'r') : 1\n",
      "('n', 'e') : 1\n",
      "('e', 'w') : 1\n",
      "('w', 'i') : 1\n",
      "('i', 'd') : 1\n",
      "('d', 'e') : 1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "def count_adjacent_pairs(token_sequences: Iterable[list[str]]) -> Counter[tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Iterate through each token sequence and count adjacent token pairs.\n",
    "    \"\"\"\n",
    "    pair_counts: Counter[tuple[str, str]] = Counter()\n",
    "    for tokens in token_sequences:\n",
    "        tokens = list(tokens)  # in case input is a tuple\n",
    "        for i in range(len(tokens) - 1):\n",
    "            pair = (tokens[i], tokens[i + 1])\n",
    "            pair_counts[pair] += 1\n",
    "    return pair_counts\n",
    "\n",
    "\n",
    "toy_pair_counts = count_adjacent_pairs(toy_tokens)\n",
    "for pair, freq in toy_pair_counts.most_common():\n",
    "    print(pair, \":\", freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc3c4ed",
   "metadata": {},
   "source": [
    "### Step 4 · merge_pair\n",
    "    Replace every occurrence of the chosen pair with a newly merged token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffa69aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['l', 'o', 'w'] → ['lo', 'w']\n",
      "['l', 'o', 'w', 'e', 'r'] → ['lo', 'w', 'e', 'r']\n",
      "['n', 'e', 'w', 'e', 's', 't'] → ['n', 'e', 'w', 'e', 's', 't']\n",
      "['w', 'i', 'd', 'e', 's', 't'] → ['w', 'i', 'd', 'e', 's', 't']\n"
     ]
    }
   ],
   "source": [
    "def merge_pair(token_sequences: list[list[str]], pair: tuple[str, str]) -> list[list[str]]:\n",
    "    \"\"\"Replace occurrences of `pair` with the\n",
    "    concatenated token pair[0]+pair[1].\"\"\"\n",
    "    merged_sequences: list[list[str]] = []\n",
    "    merged_token = pair[0] + pair[1]\n",
    "\n",
    "    for tokens in token_sequences:\n",
    "        i = 0\n",
    "        new_tokens: list[str] = []\n",
    "        while i < len(tokens):\n",
    "            if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i + 1] == pair[1]:\n",
    "                new_tokens.append(merged_token)\n",
    "                i += 2  # skip both tokens that were merged\n",
    "            else:\n",
    "                new_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "        merged_sequences.append(new_tokens)\n",
    "\n",
    "    return merged_sequences\n",
    "\n",
    "\n",
    "demo_pair = (\"l\", \"o\")\n",
    "merged_once = merge_pair(toy_tokens, demo_pair)\n",
    "for before, after in zip(toy_tokens, merged_once):\n",
    "    print(before, \"→\", after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2d99da",
   "metadata": {},
   "source": [
    "### Step 5 · First Merge Loop\n",
    "     Tie together the helper functions to perform a single merge iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e58e070a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best pair: ('l', 'o') frequency: 2\n",
      "After first merge:\n",
      "['lo', 'w']\n",
      "['lo', 'w', 'e', 'r']\n",
      "['n', 'e', 'w', 'e', 's', 't']\n",
      "['w', 'i', 'd', 'e', 's', 't']\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "working_tokens = deepcopy(toy_tokens)\n",
    "\n",
    "pair_counts = count_adjacent_pairs(working_tokens)\n",
    "best_pair = max(pair_counts, key=pair_counts.get)\n",
    "print(\"Best pair:\", best_pair, \"frequency:\", pair_counts[best_pair])\n",
    "\n",
    "working_tokens = merge_pair(working_tokens, best_pair)\n",
    "print(\"After first merge:\")\n",
    "for tokens in working_tokens:\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707bd239",
   "metadata": {},
   "source": [
    "### Step 6 · build_vocab\n",
    "     Collect the base symbols and newly merged symbols into a simple vocab map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f92bd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 : l\n",
      " 1 : o\n",
      " 2 : w\n",
      " 3 : e\n",
      " 4 : r\n",
      " 5 : n\n",
      " 6 : s\n",
      " 7 : t\n",
      " 8 : i\n",
      " 9 : d\n",
      "10 : lo\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def build_vocab(base_sequences: list[list[str]], merges: list[tuple[str, str]]):\n",
    "    \"\"\"\n",
    "    Return an OrderedDict mapping token string -> integer id in the order they are introduced.\n",
    "    \"\"\"\n",
    "    vocab = OrderedDict()\n",
    "\n",
    "    # Seed with all base tokens (characters) in appearance order.\n",
    "    for seq in base_sequences:\n",
    "        for token in seq:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = len(vocab)\n",
    "\n",
    "    # Append newly created tokens following merge order.\n",
    "    for pair in merges:\n",
    "        merged_token = pair[0] + pair[1]\n",
    "        if merged_token not in vocab:\n",
    "            vocab[merged_token] = len(vocab)\n",
    "\n",
    "    return vocab\n",
    "\n",
    "\n",
    "demo_merges = [best_pair]  # reuse the best_pair from Step 5 demo\n",
    "vocab_preview = build_vocab(toy_tokens, demo_merges)\n",
    "\n",
    "for token, idx in vocab_preview.items():\n",
    "    print(f\"{idx:>2} : {token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sf2ju1i8he8",
   "metadata": {},
   "source": [
    "### Step 7 · train_bpe (full loop)\n",
    "    Combine all helper functions into the full training routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99h6q41ql5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merges:\n",
      " 1: ('l', 'o')\n",
      " 2: ('lo', 'w')\n",
      " 3: ('e', 's')\n",
      " 4: ('es', 't')\n",
      " 5: ('low', 'e')\n",
      "\n",
      "Final token sequences:\n",
      "['low']\n",
      "['lowe', 'r']\n",
      "['n', 'e', 'w', 'est']\n",
      "['w', 'i', 'd', 'est']\n",
      "\n",
      "Vocab preview (first 12 entries):\n",
      " 0 : l\n",
      " 1 : o\n",
      " 2 : w\n",
      " 3 : e\n",
      " 4 : r\n",
      " 5 : n\n",
      " 6 : s\n",
      " 7 : t\n",
      " 8 : i\n",
      " 9 : d\n",
      "10 : lo\n",
      "11 : low\n"
     ]
    }
   ],
   "source": [
    "def train_bpe(corpus: list[str], max_merges: int):\n",
    "    \"\"\"\n",
    "    Train a BPE tokenizer on the given corpus.\n",
    "    Returns: (merges, vocab, final_tokens)\n",
    "    \"\"\"\n",
    "    base_tokens = tokenize_to_chars(corpus)\n",
    "    working_tokens = [seq[:] for seq in base_tokens]  # deep copy\n",
    "    merges: list[tuple[str, str]] = []\n",
    "\n",
    "    for _ in range(max_merges):\n",
    "        pair_counts = count_adjacent_pairs(working_tokens)\n",
    "        if not pair_counts:\n",
    "            break\n",
    "\n",
    "        best_pair = max(pair_counts, key=pair_counts.get)\n",
    "        merges.append(best_pair)\n",
    "        working_tokens = merge_pair(working_tokens, best_pair)\n",
    "\n",
    "    vocab = build_vocab(base_tokens, merges)\n",
    "    return merges, vocab, working_tokens\n",
    "\n",
    "\n",
    "# Train BPE on our toy corpus\n",
    "merges, vocab, final_tokens = train_bpe(toy_corpus, max_merges=5)\n",
    "\n",
    "print(\"Merges:\")\n",
    "for i, pair in enumerate(merges, 1):\n",
    "    print(f\"{i:>2}: {pair}\")\n",
    "\n",
    "print(\"\\nFinal token sequences:\")\n",
    "for tokens in final_tokens:\n",
    "    print(tokens)\n",
    "\n",
    "print(\"\\nVocab preview (first 12 entries):\")\n",
    "for i, (token, idx) in enumerate(vocab.items()):\n",
    "    if i >= 12:\n",
    "        break\n",
    "    print(f\"{idx:>2} : {token}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
